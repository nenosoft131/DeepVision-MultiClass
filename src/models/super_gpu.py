import os
import numpy as np
import pandas as pd
from itertools import product
from tqdm import tqdm
import time
import gc
import tensorflow as tf
K = tf.keras.backend
from logger_config import get_logger
from PreProcessPipeline import start_build_TT_separate
from data_generator import DataGenerator
from utils import Utils
from ML_Data_Analyais_Toolkit.src.models.conf import config
import torch.nn as nn
class RunPipeline():
    def __init__(self,
                 suffix: str=None,
                 mode: str='rla',
                 parallel: str=None,
                 generate_duplicates: bool=False,
                 n_samples_lower_bound: int=1000,
                 n_samples_upper_bound: int=900000):
        
        self.logger = get_logger(__name__)
        # utils function
        self.utils = Utils()
        self.mode = mode
        self.parallel = parallel
        self.score_train = None
        
        # global parameters
        self.generate_duplicates = generate_duplicates
        self.n_samples_lower_bound = n_samples_lower_bound
        self.n_samples_upper_bound = n_samples_upper_bound


        # the suffix of all saved files
        self.suffix = suffix + '-' + self.parallel

        if not os.path.exists('result'):
            os.makedirs('result')

        # data generator instantiation
        self.data_generator = DataGenerator(generate_duplicates=self.generate_duplicates,
                                            n_samples_lower_bound=self.n_samples_lower_bound,
                                            n_samples_upper_bound=self.n_samples_upper_bound)

        if self.parallel != 'unsupervise':
            # ratio of labeled anomalies
            self.rla_list = [0.05, 0.10, 0.20]
            # number of labeled anomalies
            self.nla_list =  [20]
        else:
            self.rla_list = [0.0]
            self.nla_list = [0]

        # seed list
        self.seed_list = list(np.arange(5) + 1)

        # model_dict (model_name: clf)
        self.model_dict = {}

        # unsupervised algorithms
        if self.parallel == 'unsupervise':
            from baseline.PyOD import PYOD
            from baseline.DAGMM.run import DAGMM

            # from pyod
            # for _ in ['IForest', 'OCSVM', 'CBLOF', 'COF', 'COPOD', 'ECOD', 'FeatureBagging', 'HBOS', 'KNN', 'LODA',
            #           'LOF', 'LSCP', 'MCD', 'PCA', 'SOD', 'SOGAAL', 'MOGAAL']:
            for _ in ['HBOS']:
                self.model_dict[_] = PYOD

            # for _ in ['IForest', 'ECOD', 'DeepSVDD']:
            #     self.model_dict[_] = PYOD

            # # DAGMM
            #     self.model_dict['DAGMM'] = DAGMM


        # semi-supervised algorithms
        elif self.parallel == 'semi-supervise':
            from baseline.PyOD import PYOD
            from baseline.GANomaly.run import GANomaly
            from baseline.DeepSAD.src.run import DeepSAD
            from baseline.REPEN.run import REPEN
            from baseline.DevNet.run import DevNet
            from baseline.PReNet.run import PReNet
            from baseline.FEAWAD.run import FEAWAD

            # self.model_dict = {'DevNet': DevNet}
            
            self.model_dict = {'GANomaly': GANomaly,
                               'DeepSAD': DeepSAD,
                               'DevNet': DevNet,
                               'PReNet': PReNet,
                               'XGBOD': PYOD}
            
             #    'REPEN': REPEN,
             #    'FEAWAD': FEAWAD,
           
        # fully-supervised algorithms
        elif self.parallel == 'supervise':
            from baseline.SVM import SVM_GPU
            # from baseline.FTTransformer.run import FTTransformer

            # from sklearn
            # for _ in ['LR', 'NB', 'SVM', 'MLP', 'RF', 'LGB', 'XGB', 'CatB']:
            for _ in ['SVM']:
                self.model_dict[_] = SVM_GPU
            # ResNet and FTTransformer for tabular data
            # for _ in ['ResNet', 'FTTransformer']:
            #     self.model_dict[_] = FTTransformer

        else:
            raise NotImplementedError

        # We remove the following model for considering the computational cost
        for _ in ['SOGAAL', 'MOGAAL', 'LSCP', 'MCD', 'FeatureBagging']:
            if _ in self.model_dict.keys():
                self.model_dict.pop(_)

    # dataset filter for delelting those datasets that do not satisfy the experimental requirement
    def dataset_filter(self):
        # dataset list in the current folder
        dataset_list_org = [os.path.splitext(_)[0] for _ in os.listdir('datasets')
                            if os.path.splitext(_)[1] == '.npz']
        
        dataset_list, dataset_size = [], []
        for dataset in dataset_list_org:
            add = True
            for seed in self.seed_list:
                
                self.data_generator.seed = seed
                self.data_generator.dataset = dataset

                try:
                    data = self.data_generator.generator(la=1.00, at_least_one_labeled=True)
                except:
                    print(f"ERROR")
                    add = False
                    pass
                    continue

                if not self.generate_duplicates and len(data['y_train']) + len(data['y_test']) < self.n_samples_lower_bound:
                    add = False

                else:
                    if self.mode == 'nla' and sum(data['y_train']) >= self.nla_list[-1]:
                        pass

                    elif self.mode == 'rla' and sum(data['y_train']) > 0:
                        pass

                    else:
                        add = False

            if add:
                dataset_list.append(dataset)
                dataset_size.append(len(data['y_train']) + len(data['y_test']))
            else:
                print(f"remove the dataset {dataset}")

        # sort datasets by their sample size
        dataset_list = [dataset_list[_] for _ in np.argsort(np.array(dataset_size))]

        return dataset_list

    # model fitting function
    def model_fit(self,model_name,params):
        self.score_train = None
        config_search_space = config(self=None, model=model_name, max_combinations=120)
        for config in config_search_space:
            try:
                self.model_name = model_name
                self.clf = self.model_dict[self.model_name]
                if self.model_name in ['DevNet', 'FEAWAD', 'REPEN']:
                    self.clf = self.clf(seed=self.seed, model_name=self.model_name, save_suffix=self.suffix)
                else:
                    self.clf = self.clf(seed=self.seed, model_name=self.model_name)

            except Exception as error:
                get_logger(__name__).info("Error....")
                print(f'Error in model initialization. Model:{self.model_name}, Error: {error}')
                pass
            try:
                # fitting
                start_time = time.time()
                
                if model_name == 'RF':
                    self.clf = self.clf.fit(X_train=self.data['X_train'], y_train=self.data['y_train'],
                                    class_weight=config['class_weight'], n_estimators=config['n_estimators'], 
                                    max_depth= config['max_depth'], min_samples_split=config['min_samples_split'],
                                    min_samples_leaf= config['min_samples_leaf'], max_features=config['max_features'],
                                    ccp_alpha=config['ccp_alpha'])
                    
                elif model_name == 'XGB':
                    self.clf = self.clf.fit(X_train=self.data['X_train'], y_train=self.data['y_train'],
                                        learning_rate=config['learning_rate'],max_depth=config['max_depth'],
                                        n_estimators=config['n_estimators'],reg_alpha=config['reg_alpha'],
                                        reg_lambda=config['reg_lambda'])
                elif model_name == 'SVM':
                    self.clf = self.clf.fit(X_train=self.data['X_train'], y_train=self.data['y_train'], 
                                        kernel=config['kernel'], gamma=config['gamma'],
                                        coef0=config['coef0'], tol=config['tol'], C=config['C'])
                elif model_name == 'LR':
                    self.clf = self.clf.fit(X_train=self.data['X_train'], y_train=self.data['y_train'],
                                        penalty=config['penalty'], C=config['C'], class_weight= config['class_weight'], 
                                        solver=config['solver'])
                elif model_name == 'MLP':
                    self.clf = self.clf.fit(X_train=self.data['X_train'], y_train=self.data['y_train'],
                                        act_fun=config['act_fun'], batch_size=config['batch_size'], 
                                        solver= config['solver'], hidden_layer_sizes=config['hidden_layer_sizes'],
                                        alpha= config['alpha'], max_iter=config['max_iter'])
                
                
                anomaly_rate = 0.17
                
                end_time = time.time(); time_fit = end_time - start_time
                # predicting score (inference)
                start_time = time.time()
                get_logger(__name__).info("Calculating Score")
                if self.score_train is None:
                    self.score_train = self.clf.predict_train_score(self.data['X_train'])
                    print("Score train calcualted")
                else:
                    get_logger(__name__).info("No score train needed")
                score_test , score_val = self.clf.predict_score(self.data['X_test'], self.data['X_val'])
                    
                get_logger(__name__).info(f"score train{self.score_train}")
                
                end_time = time.time(); time_inference = end_time - start_time
                result = self.utils.metric(self.score_train, anomaly_rate, y_true=self.data['y_test'], y_score=score_test, val_true=self.data['y_val'] , val_score=score_val, pos_label=1)
                
                K.clear_session()
                gc.collect()                
                df_AUCROC = pd.DataFrame({'model_name': [model_name], 'params': [params], 'config': [config],
                'AUCROC-Val':        [result['val_aucroc']],
                'AUCPR-Val':         [result['val_aucpr']],
                'Accuracy-Val':      [result['val_accuracy']],
                'Precision-Val':     [result['val_precision']],
                'Recall-Val':        [result['val_recall']],
                'F1-Val':            [result['val_f1']],
                'F1-Macro-Val':      [result['val_f1_macro']],
                'F1-Micro-Val':      [result['val_f1_micro']],
                'Confusion-Matrix-Val': [result['val_confusion_matrix']],

                'AUCROC-Test':       [result['test_aucroc']],
                'AUCPR-Test':        [result['test_aucpr']],
                'Accuracy-Test':     [result['test_accuracy']],
                'Precision-Test':    [result['test_precision']],
                'Recall-Test':       [result['test_recall']],
                'F1-Test':           [result['test_f1']],
                'F1-Macro-Test':     [result['test_f1_macro']],
                'F1-Micro-Test':     [result['test_f1_micro']],
                'Confusion-Matrix-Test': [result['test_confusion_matrix']],
                'TIME_FIT' :[time_fit], 'TIME_INFER':[time_inference]})
                
                
                file_path = os.path.join(os.getcwd(), 'result', 'AUCROC-' + self.suffix + '_GPU.csv')
                df_AUCROC.to_csv(file_path, mode='a', index=True, header=not os.path.exists(file_path))
            except Exception as error:
                print(f'Error in model fitting. Model:{self.model_name}, Error: {error}')
                time_fit, time_inference = None, None
                result = {'aucroc': np.nan, 'aucpr': np.nan}
                pass
        
        return time_fit, time_inference, result

    # run the experiment
    def run(self):
        dataset_list = self.dataset_filter()
        if self.mode == 'nla':
            experiment_params = list(product(dataset_list, self.seed_list, self.nla_list))
        else:
            experiment_params = list(product(dataset_list, self.seed_list, self.rla_list))

        print(f'{len(dataset_list)} datasets, {len(self.model_dict.keys())} models')

        for i, params in tqdm(enumerate(experiment_params)):
            dataset, self.seed, la = params

            if self.parallel == 'unsupervise' and la != 0.0:
                continue

            print(f'Current experiment parameters: {params}')

            # generate data
            self.data_generator.seed = self.seed
            self.data_generator.dataset = dataset

            try:
                self.data = self.data_generator.generator(la=100, at_least_one_labeled=True)

            except Exception as error:
                print(f'Error when generating data: {error}')
                pass
                continue
            for model_name in tqdm(self.model_dict.keys()):
                time_fit, time_inference, result = self.model_fit(model_name, params)

                
get_logger(__name__).info("Reading Data")
pipeline = RunPipeline(suffix='SOTA', parallel='supervise', mode='nla')
pipeline.run()